{
  "query": "tantivy index",
  "limit": 10,
  "offset": 0,
  "count": 10,
  "total_matches": 10,
  "hits": [
    {
      "title": "Search the tantivy crate source code in the cargo target directory or in the Cargo registry for the ",
      "snippet": "[Tool: Read - /home/ubuntu/.cargo/registry/src/**index**.crates.io-1949cf8c6b5b557f/**tantivy**-0.25.0/examples/snippet.rs",
      "content": "[Tool: Read - /home/ubuntu/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/tantivy-0.25.0/examples/snippet.rs]",
      "score": 46.91132736206055,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/40dd9a37-236e-41af-bc9f-fc18de1e4aa8/subagents/agent-a5018aa.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771031191307,
      "line_number": 18,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Search the tantivy crate source code in the cargo target directory or in the Cargo registry for the ",
      "snippet": "[Tool: Read - /home/ubuntu/.cargo/registry/src/**index**.crates.io-1949cf8c6b5b557f/**tantivy**-0.25.0/src/snippet/mod.rs",
      "content": "[Tool: Read - /home/ubuntu/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/tantivy-0.25.0/src/snippet/mod.rs]",
      "score": 46.90912628173828,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/40dd9a37-236e-41af-bc9f-fc18de1e4aa8/subagents/agent-a5018aa.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771031190929,
      "line_number": 17,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Search the frankensearch project and its dependencies to understand Tantivy's segment management API",
      "snippet": "**Tantivy** **Index** struct and its public methods related to segments\n\nLook in /data/projects/frankensearch and the Cargo.lock or **tantivy** source in the cargo registry",
      "content": "Search the frankensearch project and its dependencies to understand Tantivy's segment management API. Specifically find:\n1. How to enumerate segments (searchable_segment_metas, searchable_segments, etc.)\n2. How segment IDs work (SegmentId, segment_id())\n3. How to list the component files for a segment\n4. The Tantivy Index struct and its public methods related to segments\n\nLook in /data/projects/frankensearch and the Cargo.lock or tantivy source in the cargo registry. Focus on Tantivy v0.22.x API.",
      "score": 45.526031494140625,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/408d0b00-7f45-4a4b-8d28-6f07c4f5e91a/subagents/agent-a3a83d7.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771039466927,
      "line_number": 1,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Research the Tantivy schema and analyzer setup in the reference implementations. I need to understan",
      "snippet": "search**: `**tantivy** = &quot;*&quot;` (latest version, unspecified)\n- **xf**: `**tantivy** = &quot;0.22&quot;` (fixed version)\n- **mcp_agent_mail_rust workspace**: No **tantivy** in Cargo",
      "content": "Perfect! Now I have all the information I need. Let me create a comprehensive report.\n\n## Tantivy Schema and Analyzer Research Report\n\nBased on my analysis of the reference implementations in `/dp/coding_agent_session_search` and `/dp/xf`, here are the key findings:\n\n### 1. Tantivy Version\n\n- **coding_agent_session_search**: `tantivy = \"*\"` (latest version, unspecified)\n- **xf**: `tantivy = \"0.22\"` (fixed version)\n- **mcp_agent_mail_rust workspace**: No tantivy in Cargo.toml yet; has feature flag for `tantivy` in search-core crate\n\n### 2. Schema Field Definitions\n\n#### coding_agent_session_search Schema (File: `/dp/coding_agent_session_search/src/search/tantivy.rs` lines 386-421)\n\n**Field Configuration:**\n\n| Field | Type | Flags | Tokenizer | Index Options | Purpose |\n|-------|------|-------|-----------|---|---------|\n| agent | text | STRING\\|STORED | N/A (exact match) | - | Agent slug, non-tokenized |\n| workspace | text | STRING\\|STORED | N/A (exact match) | - | Workspace path, non-tokenized |\n| workspace_original | text | STORED | N/A | - | Pre-rewrite path for audit |\n| source_path | text | STORED | N/A | - | Source file path |\n| msg_idx | u64 | INDEXED\\|STORED | N/A | - | Message index |\n| created_at | i64 | INDEXED\\|STORED\\|FAST | N/A | - | Timestamp for sorting (FAST = columnar) |\n| title | text | TEXT + stored | hyphen_normalize | WithFreqsAndPositions | Full-text searchable, stored |\n| content | text | TEXT + stored | hyphen_normalize | WithFreqsAndPositions | Full-text searchable, stored |\n| title_prefix | text | TEXT (not stored) | hyphen_normalize | WithFreqsAndPositions | Edge n-gram prefix matching |\n| content_prefix | text | TEXT (not stored) | hyphen_normalize | WithFreqsAndPositions | Edge n-gram prefix matching |\n| preview | text | TEXT\\|STORED | default | - | 400-char excerpt |\n| source_id | text | STRING\\|STORED | N/A | - | Provenance field, exact match |\n| origin_kind | text | STRING\\|STORED | N/A | - | Provenance field, exact match |\n| origin_host | text | STRING\\|STORED | N/A | - | Provenance field, exact match |\n\n#### xf Schema (File: `/dp/xf/src/search.rs` lines 213-248)\n\n| Field | Type | Flags | Tokenizer | Index Options | Purpose |\n|-------|------|-------|-----------|---|---------|\n| id | text | STRING\\|STORED | N/A | - | Document ID, exact match |\n| text | text | TEXT + stored | default | WithFreqsAndPositions | Main full-text search field |\n| text_prefix | text | TEXT (not stored) | default | Basic | Prefix n-gram matching |\n| type | text | STRING\\|STORED | N/A | - | Document type filter |\n| created_at | i64 | INDEXED\\|STORED\\|FAST | N/A | - | Timestamp for sorting |\n| metadata | text | STORED | N/A | - | JSON metadata storage |\n\n### 3. Tokenizer Chain Configuration\n\n#### coding_agent_session_search (lines 475-482)\n\n```rust\npub fn ensure_tokenizer(index: &mut Index) {\n    use tantivy::tokenizer::{LowerCaser, RemoveLongFilter, SimpleTokenizer, TextAnalyzer};\n    let analyzer = TextAnalyzer::builder(SimpleTokenizer::default())\n        .filter(LowerCaser)\n        .filter(RemoveLongFilter::limit(256))\n        .build();\n    index.tokenizers().register(\"hyphen_normalize\", analyzer);\n}\n```\n\n**Custom Tokenizer Chain \"hyphen_normalize\":**\n1. **Base Tokenizer**: `SimpleTokenizer::default()` - splits on whitespace and punctuation\n2. **Filter 1**: `LowerCaser` - converts all text to lowercase\n3. **Filter 2**: `RemoveLongFilter::limit(256)` - removes tokens longer than 256 bytes\n\n#### xf\n\nUses the **default tokenizer** (no custom tokenizer registration):\n- Standard Tantivy tokenizer with whitespace/punctuation splitting\n- Lowercase conversion\n- No custom filters\n\n### 4. Schema Versioning and Hashing\n\n#### coding_agent_session_search (lines 17, 52)\n\n```rust\nconst SCHEMA_VERSION: &str = \"v6\";\npub const SCHEMA_HASH: &str = \"tantivy-schema-v6-long-tokens\";\n```\n\n**Versioning Strategy:**\n- SCHEMA_VERSION directory: `index/v6/` (used for path organization)\n- SCHEMA_HASH: Descriptive hash string changed when schema or tokenizer changes\n- Hash includes: version number + tokenizer name (\"long-tokens\" descriptor)\n- Stored in `schema_hash.json` file within index directory\n- If stored hash ≠ SCHEMA_HASH, entire index is rebuilt (lines 85-105)\n- Hash check happens on every `open_or_create()` call\n\n### 5. Index Opening and Rebuild Logic (lines 85-144)\n\n```\n1. Calculate schema hash from source code (SCHEMA_HASH constant)\n2. Check if schema_hash.json exists and matches\n3. If mismatch → needs_rebuild = true\n4. If needs_rebuild → delete entire index dir, create fresh\n5. Open or create index with schema\n6. Register custom tokenizer via ensure_tokenizer()\n7. Write current SCHEMA_HASH to schema_hash.json for future opens\n8. Use schema from actual index (not rebuilt version) to derive Field IDs\n```\n\n### 6. Boost Strategy\n\n**coding_agent_session_search**: No explicit boost configuration found in schema or tokenizer setup.\n- Uses BM25 default scoring (Tantivy default)\n- Separate ranking possible via field-specific queries\n\n**xf**: No explicit boost configuration found.\n- Uses BM25 default scoring\n- Snippet generation for highlighting\n\n### 7. Key Patterns for Implementation\n\n**For mcp_agent_mail_rust:**\n\n1. **Register a custom tokenizer** before opening the index:\n```rust\npub fn ensure_tokenizer(index: &mut Index) {\n    use tantivy::tokenizer::{LowerCaser, RemoveLongFilter, SimpleTokenizer, TextAnalyzer};\n    let analyzer = TextAnalyzer::builder(SimpleTokenizer::default())\n        .filter(LowerCaser)\n        .filter(RemoveLongFilter::limit(256))\n        .build();\n    index.tokenizers().register(\"hyphen_normalize\", analyzer);\n}\n```\n\n2. **Use separate fields for different index strategies**:\n   - Full-text fields (title, content) with `WithFreqsAndPositions`\n   - Prefix fields (*_prefix) with `Basic` IndexRecordOption for lower overhead\n   - Exact-match fields (agent, source_id) using STRING type\n\n3. **Implement schema versioning** by:\n   - Creating a SCHEMA_HASH constant combining version + tokenizer info\n   - Storing hash in JSON file alongside index\n   - Comparing on every open; rebuild if mismatch\n\n4. **Use FAST flag** on i64 timestamp field for efficient range queries and sorting\n\n5. **Schema fields match document types**: coding_agent_session_search indexes agent conversations; xf indexes tweets/likes/DMs\n\n### 8. Workspace Dependencies Status\n\nCurrently in `/data/projects/mcp_agent_mail_rust/Cargo.toml`:\n- No tantivy in workspace.dependencies\n- Search-core crate exists with `[features] tantivy = []`\n- Ready to add tantivy integration via feature flag\n\n**Recommended addition to workspace.dependencies:**\n```toml\ntantivy = \"0.22\"\n```\n\nThis matches xf's pinned version for stability.",
      "score": 44.03019714355469,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-mcp-agent-mail-rust/07c34790-1305-4a7d-b54b-c6e626c32f16/subagents/agent-a960638.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/mcp_agent_mail_rust",
      "created_at": 1770872506279,
      "line_number": 31,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Search the frankensearch project and its dependencies to understand Tantivy's segment management API",
      "snippet": "**index**/struct.**Index**.html)\n- [**Tantivy** SegmentMeta struct documentation](https://docs.rs/**tantivy**/latest/**tantivy**/**index**/struct.SegmentMeta.html)\n- [**Tantivy** Segment",
      "content": "Perfect! Now I have all the information needed. Let me create a comprehensive report:\n\n## Summary: Tantivy v0.22.1 Segment Management API\n\nI've thoroughly explored the Tantivy segment management API as used in the frankensearch project. Here's a comprehensive breakdown:\n\n### 1. **How to Enumerate Segments**\n\nThe primary methods on `Index` for discovering segments are:\n\n- **`searchable_segments() -> Vec<Segment>`** - Returns the list of segment objects that are currently searchable\n- **`searchable_segment_ids() -> Vec<SegmentId>`** - Returns just the segment identifiers for all searchable segments\n- **`searchable_segment_metas() -> Vec<SegmentMeta>`** - Reads the `meta.json` file and returns complete segment metadata objects from the last commit\n- **`load_metas() -> IndexMeta`** - Reads and returns the entire index meta file containing all segment metadata\n\n### 2. **How Segment IDs Work**\n\n**SegmentId** is a UUID-based wrapper type:\n\n```rust\n#[derive(Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct SegmentId(Uuid);\n```\n\n**Key Methods on SegmentId:**\n- `generate_random() -> SegmentId` - Creates a new random segment identifier\n- `short_uuid_string() -> String` - Returns first 8 characters of the UUID (e.g., \"a5c4dfcb\") for display\n- `uuid_string() -> String` - Returns full 32-character lowercase hex UUID\n- `from_uuid_string(uuid_str: &str) -> Result<SegmentId, ...>` - Parses a UUID string into SegmentId\n- `FromStr` trait implementation for parsing from strings\n- Implements `Ord` for sorting and comparison operations\n\nAll segment files use the naming convention: `segment_uuid.component_extension` (or `segment_uuid.delete_opstamp.component_extension` for delete files).\n\n### 3. **How to List Component Files for a Segment**\n\nThe **SegmentMeta** object provides file enumeration:\n\n- **`list_files() -> HashSet<PathBuf>`** - Returns all files required for the segment as a set of PathBuf objects\n- **`relative_path(component: SegmentComponent) -> PathBuf`** - Returns the relative path of a specific component by joining the segment ID with the component's file extension\n\nThe **Segment** object offers direct file access:\n\n- **`meta() -> &SegmentMeta`** - Access the segment's metadata\n- **`relative_path(component: SegmentComponent) -> PathBuf`** - Get the path for a component\n- **`open_read(component: SegmentComponent) -> Result<FileSlice, ...>`** - Opens a component file for reading\n- **`open_write(component: SegmentComponent) -> Result<WritePtr, ...>`** - Opens a component file for writing\n\n### 4. **Segment Component Types (SegmentComponent Enum)**\n\nThere are 8 distinct file types in each segment:\n\n1. **Postings** - Sorted lists of document IDs associated with terms\n2. **Positions** - Positions of terms within each document (for phrase queries)\n3. **FastFields** - Column-oriented random-access storage for field values\n4. **FieldNorms** - Sum of field term lengths for each document\n5. **Terms** - Dictionary mapping Terms to TermInfo (offsets into postings/positions files)\n6. **Store** - Row-oriented compressed storage of document fields\n7. **TempStore** - Temporary storage before being streamed to Store\n8. **Delete** - Bitset describing which documents are alive (note: v0.17+ uses \"alive\" not \"deleted\")\n\n### 5. **The Tantivy Index Struct - Public Methods for Segments**\n\n**Reading Segments:**\n- `searchable_segment_metas() -> Vec<SegmentMeta>` - Core method for enumeration\n- `searchable_segment_ids() -> Vec<SegmentId>` - Get just the IDs\n- `searchable_segments() -> Vec<Segment>` - Get full Segment objects\n- `new_segment() -> Segment` - Create a fresh segment instance for indexing\n- `new_segment_meta(segment_id: SegmentId) -> SegmentMeta` - Construct metadata\n\n**Configuration:**\n- `set_executor(executor)` - Configure thread pool for segment search\n- `set_multithread_executor()` - Use multi-threaded search across segments\n- `fields_metadata()` - Access field information including JSON fields\n\n### 6. **The Segment Struct - Key Methods**\n\n- `index() -> &Index` - Reference to parent index\n- `schema() -> &Schema` - Index schema\n- `id() -> SegmentId` - Get segment's unique identifier\n- `meta() -> &SegmentMeta` - Access segment metadata\n- `relative_path(component: SegmentComponent) -> PathBuf` - Get path for a component\n- `open_read(component) -> Result<FileSlice>` - Read a component file\n- `open_write(component) -> Result<WritePtr>` - Write a component file\n\n### 7. **The SegmentMeta Struct - Complete Interface**\n\n**Identification & Counting:**\n- `id() -> SegmentId` - Returns the segment's unique identifier\n- `max_doc() -> DocId` - Highest document ID + 1\n- `num_docs() -> u32` - Total documents in segment\n- `num_deleted_docs() -> u32` - Count of deleted documents\n\n**Deletion Tracking:**\n- `delete_opstamp() -> Opstamp` - Timestamp of last delete operation\n- `has_deletes() -> bool` - Whether segment has any deletions\n\n**File Management:**\n- `list_files() -> HashSet<PathBuf>` - All files required for the segment\n- `relative_path(component: SegmentComponent) -> PathBuf` - Path for a specific component\n- `untrack_temp_docstore()` - Remove TempStore from active components\n\n**Modification:**\n- `with_max_doc(max_doc: DocId) -> SegmentMeta` - Create new metadata with updated max_doc\n\n### 8. **Searcher Integration**\n\nThe **Searcher** provides segment-level access:\n\n- `segment_readers() -> Vec<SegmentReader>` - Get all segment readers in this search snapshot\n- `segment_reader(ord: u32) -> &SegmentReader` - Access a specific segment by ordinal\n- `num_docs()` - Total docs aggregated across all segments\n- `doc_freq(term) -> u64` - Document frequency aggregated across segments\n\n### 9. **Example Usage Pattern (from frankensearch-lexical)**\n\nFrom `/data/projects/frankensearch/crates/frankensearch-lexical/src/lib.rs`:\n\n```rust\n// Create or open an index\nlet index = Index::create_in_dir(path, schema)?;\n\n// Get a reader for searching\nlet reader = index.reader_builder().reload_policy(...).try_into()?;\nlet searcher = reader.searcher();\n\n// Access segments\nlet segment_metas = index.searchable_segment_metas();\nfor meta in segment_metas {\n    let segment_id = meta.id();\n    let files = meta.list_files();\n    let doc_count = meta.num_docs();\n}\n\n// Use searcher with segments\nlet top_docs = searcher.search(&query, &TopDocs::with_limit(10))?;\n```\n\n### 10. **Integration with Durability (bd-3w1.8 context)**\n\nThe frankensearch project's durability layer (`frankensearch-durability`) provides:\n\n- **TantivySegmentProtector** - Helper for applying RaptorQ durability sidecars to segment files\n- **FileProtector** - Protects, verifies, and repairs individual segment component files\n- Methods to enumerate all segment files and create `.fec` sidecar files for each\n\n**File naming for durability:**\n- Original: `segment_uuid.component_ext`\n- Sidecar: `segment_uuid.component_ext.fec` (RaptorQ repair symbols)\n- Delete: `segment_uuid.delete_opstamp.component_ext`\n- Delete sidecar: `segment_uuid.delete_opstamp.component_ext.fec`\n\n### 11. **Key API Patterns**\n\n**Pattern 1: Enumerate all segments with metadata**\n```\nindex.searchable_segment_metas() -> Vec<SegmentMeta>\n```\n\n**Pattern 2: Get files for a segment**\n```\nsegment_meta.list_files() -> HashSet<PathBuf>\n// or for specific component:\nsegment_meta.relative_path(SegmentComponent::Store) -> PathBuf\n```\n\n**Pattern 3: Access segment files directly**\n```\nsegment.open_read(SegmentComponent::Store) -> Result<FileSlice>\n```\n\n**Pattern 4: Work with segment IDs**\n```\nsegment_id.uuid_string() -> String  // full UUID\nsegment_id.short_uuid_string() -> String  // 8-char prefix\n```\n\n---\n\n## Key Findings for Durability Integration\n\nFor the bd-3w1.8 (Tantivy segment protection) work:\n\n1. **Segment discovery** is read-only and thread-safe - no locking needed for enumeration\n2. **SegmentMeta.list_files()** returns all component files in one call - ideal for batch protection\n3. **Component types** are well-defined (8 variants) - use these for comprehensive coverage\n4. **SegmentId** is stable and hashable - suitable for correlation with sidecar files\n5. **No destructive operations** during enumeration - safe to iterate and protect during searches\n6. **File naming is deterministic** - `segment_uuid.component_ext` pattern enables parallel sidecar generation\n\n---\n\n## Sources\n\n- [Tantivy Index struct documentation](https://docs.rs/tantivy/latest/tantivy/index/struct.Index.html)\n- [Tantivy SegmentMeta struct documentation](https://docs.rs/tantivy/latest/tantivy/index/struct.SegmentMeta.html)\n- [Tantivy Segment struct documentation](https://docs.rs/tantivy/latest/tantivy/index/struct.Segment.html)\n- [Tantivy SegmentComponent enum documentation](https://docs.rs/tantivy/0.22.1/tantivy/index/enum.SegmentComponent.html)\n- [Tantivy SegmentId source code](https://docs.rs/tantivy/latest/src/tantivy/index/segment_id.rs.html)\n- [Tantivy Architecture documentation](https://github.com/quickwit-oss/tantivy/blob/main/ARCHITECTURE.md)\n- [Tantivy Searcher documentation](https://docs.rs/tantivy/0.22.1/tantivy/struct.Searcher.html)\n- [Frankensearch project: frankensearch-lexical/src/lib.rs](/data/projects/frankensearch/crates/frankensearch-lexical/src/lib.rs)\n- [Frankensearch project: frankensearch-durability/src/tantivy_wrapper.rs](/data/projects/frankensearch/crates/frankensearch-durability/src/tantivy_wrapper.rs)",
      "score": 43.08802795410156,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/408d0b00-7f45-4a4b-8d28-6f07c4f5e91a/subagents/agent-a3a83d7.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771039609436,
      "line_number": 68,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Search the tantivy crate source code in the cargo target directory or in the Cargo registry for the ",
      "snippet": "TopDocs;\nuse **tantivy**::query::QueryParser;\nuse **tantivy**::schema::*;\nuse **tantivy**::snippet::SnippetGenerator;\nuse **tantivy**::{doc, **Index**};\n\nfn main() -&gt; **tantivy**",
      "content": "Excellent! Now let me create a comprehensive summary document. Let me verify the exact signature and usage patterns once more by looking at the Snippet struct API:\n\nBased on my search of the tantivy 0.25.0 source code, here's the complete tantivy snippet API specification:\n\n## Module Path\n```rust\ntantivy::snippet\n```\n\nThe module is publicly exported at the root level: `pub mod snippet;` (line 209 in lib.rs)\n\n## Core Types\n\n### `SnippetGenerator`\n**Location**: `/home/ubuntu/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/tantivy-0.25.0/src/snippet/mod.rs` (lines 377-475)\n\n**Definition**:\n```rust\npub struct SnippetGenerator {\n    terms_text: BTreeMap<String, Score>,\n    tokenizer: TextAnalyzer,\n    field: Field,\n    max_num_chars: usize,\n}\n```\n\n### Key Methods\n\n#### Creation\n```rust\nimpl SnippetGenerator {\n    /// Creates a new snippet generator from a searcher, query, and field\n    pub fn create(\n        searcher: &Searcher,\n        query: &dyn Query,\n        field: Field,\n    ) -> crate::Result<SnippetGenerator>\n    \n    /// Creates a snippet generator with explicit parameters\n    pub fn new(\n        terms_text: BTreeMap<String, Score>,\n        tokenizer: TextAnalyzer,\n        field: Field,\n        max_num_chars: usize,\n    ) -> Self\n}\n```\n\n#### Configuration\n```rust\n/// Sets maximum number of characters (default: 150)\npub fn set_max_num_chars(&mut self, max_num_chars: usize)\n```\n\n#### Snippet Generation\n```rust\n/// Generate snippet from a Document trait object\npub fn snippet_from_doc<D: Document>(&self, doc: &D) -> Snippet\n\n/// Generate snippet from raw text string\npub fn snippet(&self, text: &str) -> Snippet\n```\n\n### `Snippet`\n**Definition** (lines 113-182):\n```rust\npub struct Snippet {\n    fragment: String,\n    highlighted: Vec<Range<usize>>,\n    snippet_prefix: String,\n    snippet_postfix: String,\n}\n```\n\n**Public Methods**:\n```rust\nimpl Snippet {\n    /// Creates empty snippet\n    pub fn empty() -> Snippet\n    \n    /// Check if snippet has no content\n    pub fn is_empty(&self) -> bool\n    \n    /// Returns highlighted HTML with <b>...</b> tags (default)\n    pub fn to_html(&self) -> String\n    \n    /// Returns the text fragment\n    pub fn fragment(&self) -> &str\n    \n    /// Returns slice of highlighted character ranges\n    pub fn highlighted(&self) -> &[Range<usize>]\n    \n    /// Sets custom prefix/postfix for highlighting (default: <b>...</b>)\n    pub fn set_snippet_prefix_postfix(&mut self, prefix: &str, postfix: &str)\n}\n```\n\n## Complete Usage Example\n\n```rust\nuse tantivy::collector::TopDocs;\nuse tantivy::query::QueryParser;\nuse tantivy::schema::*;\nuse tantivy::snippet::SnippetGenerator;\nuse tantivy::{doc, Index};\n\nfn main() -> tantivy::Result<()> {\n    // 1. Setup schema and index\n    let mut schema_builder = Schema::builder();\n    let title = schema_builder.add_text_field(\"title\", TEXT | STORED);\n    let body = schema_builder.add_text_field(\"body\", TEXT | STORED);\n    let schema = schema_builder.build();\n    let index = Index::create_in_ram(schema);\n    \n    // 2. Index documents\n    let mut index_writer = index.writer(50_000_000)?;\n    index_writer.add_document(doc!(\n        title => \"Of Mice and Men\",\n        body => \"A few miles south of Soledad, the Salinas River drops in close...\"\n    ))?;\n    index_writer.commit()?;\n    \n    // 3. Parse query\n    let reader = index.reader()?;\n    let searcher = reader.searcher();\n    let query_parser = QueryParser::for_index(&index, vec![title, body]);\n    let query = query_parser.parse_query(\"sycamore spring\")?;\n    \n    // 4. Execute search\n    let top_docs = searcher.search(&query, &TopDocs::with_limit(10))?;\n    \n    // 5. CREATE SNIPPET GENERATOR (KEY API CALL)\n    let mut snippet_generator = SnippetGenerator::create(&searcher, &*query, body)?;\n    \n    // 6. Optional: customize max characters\n    snippet_generator.set_max_num_chars(100);\n    \n    // 7. Generate snippets for each result\n    for (score, doc_address) in top_docs {\n        let doc = searcher.doc(doc_address)?;\n        \n        // Method 1: From document\n        let snippet = snippet_generator.snippet_from_doc(&doc);\n        \n        // Method 2: From raw text\n        let snippet = snippet_generator.snippet(\"some text here\");\n        \n        // 8. Render as HTML (default: <b>keyword</b>)\n        let html: String = snippet.to_html();\n        println!(\"snippet: {}\", html);\n        \n        // Optional: Custom highlighting\n        let fragment = snippet.fragment();\n        let highlighted_ranges = snippet.highlighted();\n        for range in highlighted_ranges {\n            println!(\"Highlighted: {}\", &fragment[range.clone()]);\n        }\n        \n        // Optional: Change highlight markup\n        let mut custom_snippet = snippet;\n        custom_snippet.set_snippet_prefix_postfix(\"<em>\", \"</em>\");\n        println!(\"Custom: {}\", custom_snippet.to_html());\n    }\n    \n    Ok(())\n}\n```\n\n## Key Characteristics\n\n1. **Query Integration**: `SnippetGenerator::create()` extracts search terms directly from the `Query` object via `query.query_terms()` callback\n   \n2. **Term Scoring**: Each extracted term gets an IDF-like score: `1.0 / (1.0 + doc_freq)`\n\n3. **Fragment Selection**: The algorithm:\n   - Tokenizes the text using field's configured `TextAnalyzer`\n   - Finds all fragments matching the `max_num_chars` limit containing query terms\n   - Scores each fragment by sum of term scores\n   - Selects the highest-scoring fragment\n   - Returns empty snippet if no terms found\n\n4. **Highlighting Mechanism**:\n   - Tracks character offsets (not byte offsets) of matched terms\n   - `to_html()` generates HTML with customizable prefix/postfix\n   - Default: `<b>keyword</b>`\n   - HTML entities are escaped via `htmlescape::encode_minimal`\n\n5. **Default Limits**:\n   - `max_num_chars`: 150 characters\n   - Highlight prefix: `<b>`\n   - Highlight postfix: `</b>`\n\n## Dependencies\n- Requires `tantivy::query::Query` trait object\n- Requires `tantivy::schema::Field` \n- Requires `tantivy::Searcher` for context (tokenizer + doc frequencies)\n- Uses internal `TextAnalyzer` from field schema",
      "score": 41.52273178100586,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/40dd9a37-236e-41af-bc9f-fc18de1e4aa8/subagents/agent-a5018aa.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771031209457,
      "line_number": 21,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Read these embedder and index beads using `br show <ID>` and report their FULL content (title, descr",
      "snippet": "bd-3un.15 (brute-force top-k)\n7. bd-3un.17 (**Tantivy** schema)\n8. bd-3un.18 (**Tantivy** query parsing)\n\nFor each bead, report the complete title, full description",
      "content": "Read these embedder and index beads using `br show <ID>` and report their FULL content (title, description, ALL comments, dependency lists):\n\n1. bd-3un.6 (FNV-1a hash embedder)\n2. bd-3un.7 (Model2Vec potion-128M)\n3. bd-3un.8 (FastEmbed MiniLM-L6-v2)\n4. bd-3un.13 (vector index format)\n5. bd-3un.14 (SIMD dot product)\n6. bd-3un.15 (brute-force top-k)\n7. bd-3un.17 (Tantivy schema)\n8. bd-3un.18 (Tantivy query parsing)\n\nFor each bead, report the complete title, full description text, every comment (full text), blocked_by list, and blocks list.",
      "score": 39.10302734375,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/248bc945-5259-4fcd-b764-be2123f336e7/subagents/agent-ad8963e.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771018989823,
      "line_number": 1,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Very thorough exploration. I need to compare the Tantivy lexical search implementations across THREE",
      "snippet": "Very thorough exploration. I need to compare the **Tantivy** lexical search implementations across THREE projects for the frankensearch extraction.\n\nRead and",
      "content": "Very thorough exploration. I need to compare the Tantivy lexical search implementations across THREE projects for the frankensearch extraction.\n\nRead and analyze these files:\n\n1. **cass**: `/data/projects/coding_agent_session_search/src/search/tantivy.rs` (44KB)\n   Also check `/data/projects/coding_agent_session_search/src/indexer/mod.rs`\n2. **xf**: `/data/projects/xf/src/search.rs`\n3. **mcp_agent_mail**: `/data/projects/mcp_agent_mail_rust/crates/mcp-agent-mail-search-core/src/tantivy_schema.rs`\n   AND `/data/projects/mcp_agent_mail_rust/crates/mcp-agent-mail-search-core/src/lexical_parser.rs`\n   AND `/data/projects/mcp_agent_mail_rust/crates/mcp-agent-mail-search-core/src/lexical_response.rs`\n\nFor EACH project, document:\n- Schema definition (fields, types, options like STORED/INDEXED/FAST)\n- Custom tokenizer chain (SimpleTokenizer, LowerCaser, RemoveLongFilter, edge n-grams)\n- Edge n-gram / prefix matching implementation\n- Query parsing (boolean operators, phrase detection, field boosting)\n- Subject vs body boost factor\n- Type/doc_kind filtering\n- Date/timestamp range queries\n- Snippet generation (highlighting, fragment extraction)\n- How index is created/opened (directory, RAM, mmap)\n- How documents are added (batch operations)\n- Thread safety (IndexWriter wrapping)\n\nProduce a COMPARISON TABLE of the best patterns from each project and what frankensearch should adopt.",
      "score": 38.05602264404297,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/90a3ac27-d05a-4a76-9e42-2c27aa78a955/subagents/agent-aec1842.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771014486558,
      "line_number": 1,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "Very thorough exploration. I need to compare the Tantivy lexical search implementations across THREE",
      "snippet": "comparison** (MCP pattern) |\n| ****Index** Locking** | **Tantivy** default (auto) | **Tantivy** default (auto) | **Tantivy** default (auto) | **Rely on **Tantivy**&#x27;s locking",
      "content": "Perfect. Now let me create the comprehensive comparison report. Let me compile all the findings into a structured document:\n\n## COMPREHENSIVE TANTIVY LEXICAL SEARCH COMPARISON ANALYSIS\n\nBased on thorough analysis of three projects, here is the detailed cross-project comparison:\n\n---\n\n### PROJECT 1: CASS (coding_agent_session_search)\n\n#### Schema Definition\n| Field | Type | Options | Purpose |\n|-------|------|---------|---------|\n| `agent` | TEXT | STRING + STORED | Agent slug (non-tokenized, exact match) |\n| `workspace` | TEXT | STRING + STORED | Workspace identifier |\n| `workspace_original` | TEXT | STORED | Original pre-rewrite workspace |\n| `source_path` | TEXT | STORED | File source path |\n| `msg_idx` | U64 | INDEXED + STORED | Message index |\n| `created_at` | I64 | INDEXED + STORED + FAST | Timestamp (epoch ms) |\n| `title` | TEXT | TEXT + STORED | Conversation title |\n| `content` | TEXT | TEXT + STORED | Message body |\n| `title_prefix` | TEXT | TEXT (not stored) | Edge n-gram prefixes for title |\n| `content_prefix` | TEXT | TEXT (not stored) | Edge n-gram prefixes for content |\n| `preview` | TEXT | TEXT + STORED | Truncated excerpt (400 chars) |\n| `source_id` | TEXT | STRING + STORED | Provenance: source identifier |\n| `origin_kind` | TEXT | STRING + STORED | Provenance: origin type |\n| `origin_host` | TEXT | STRING + STORED | Provenance: origin host |\n\n**Schema Version**: `tantivy-schema-v6-long-tokens` (with SHA comparison on open)\n\n#### Tokenizer Chain\n```rust\nSimpleTokenizer → LowerCaser → RemoveLongFilter::limit(256)\n```\n- **Name**: `hyphen_normalize`\n- **Token length limit**: 256 bytes\n- **Index record option**: `WithFreqsAndPositions` (enables phrase queries)\n\n#### Edge N-gram Implementation\n**Function**: `generate_edge_ngrams(text: &str) → String`\n- Uses `ArrayVec<usize, 21>` (stack-allocated, no heap allocation)\n- Splits text by non-alphanumeric boundaries\n- **Min word length**: 2 characters (single-char words skipped)\n- **Max prefix length**: 20 characters (or word length if shorter)\n- **Output format**: Space-separated prefixes (e.g., \"he hel hell hello\")\n\n**Key optimization**: ArrayVec avoids heap allocation for millions of words during bulk indexing\n\n#### Query Parsing\n- Standard `QueryParser::for_index()` for both `title` and `content` fields\n- **Phrase detection**: Not explicitly mentioned; uses standard Tantivy parsing\n- **Prefix matching**: Enabled via `*` wildcard in raw query, matched against `title_prefix` and `content_prefix` fields\n- **No custom field boosting** (boost applied at query-time if needed outside search module)\n\n#### Subject vs Body Boost\n- **Not implemented** in schema; title and content are equal weight\n- Boost could be applied at query time if needed\n\n#### Type/Doc Kind Filtering\n- Uses `agent` field (STRING/exact match) for agent slug filtering\n- Uses `origin_kind` field (STRING/exact match) for source type filtering\n- Can combine with TermQuery for exact match\n\n#### Date/Timestamp Range Queries\n- `created_at` is I64 + INDEXED + FAST\n- Stored as milliseconds since epoch\n- Can construct range queries via BooleanQuery with term ranges\n\n#### Snippet Generation\n**Function**: `build_preview(content: &str, max_chars: usize) → String`\n- Truncates to max 400 characters\n- Adds `…` if content exceeds limit\n- **No highlighting**: Plain text only\n\n#### Index Creation/Opening\n- **Path-based**: Creates `{base}/index/{SCHEMA_VERSION}/` directory\n- **Schema hash tracking**: Writes `schema_hash.json` after schema mismatch detection\n- **Lock handling**: Tantivy's automatic locking; full rebuild on hash mismatch\n- **Fallback**: If open fails, attempts full rebuild (removes and recreates directory)\n\n#### Document Addition (Batch)\n- Single `IndexWriter` for the process\n- Accumulates documents via `add_messages()` which loops through conversation messages\n- Each message becomes one document (precomputes title_prefix/content_prefix once per conversation)\n- Explicit `commit()` required for visibility to readers\n\n#### Thread Safety\n- Single mutable `writer: IndexWriter` (not thread-safe across concurrent writers)\n- Uses `Arc<Mutex<>>` pattern in indexer for multi-threaded sync\n- **Merge strategy**: Background async merge via `merge()` with cooldown (5 minutes minimum, 4 segment threshold)\n\n#### Segment Management\n- **Merge cooldown**: 300 seconds (5 minutes)\n- **Segment threshold**: 4 segments before merge triggered\n- **Merge is async**: `merge()` returns FutureResult; non-blocking\n- **Force merge available**: Synchronous wait version\n\n---\n\n### PROJECT 2: XF (xf - Twitter/X search)\n\n#### Schema Definition\n| Field | Type | Options | Purpose |\n|-------|------|---------|---------|\n| `id` | TEXT | STRING + STORED | Document ID |\n| `text` | TEXT | TEXT + STORED | Full text content |\n| `text_prefix` | TEXT | TEXT (Basic index) | Edge n-gram prefixes |\n| `type` | TEXT | STRING + STORED | Doc type (tweet, like, dm, grok) |\n| `created_at` | I64 | INDEXED + STORED + FAST | Timestamp |\n| `metadata` | TEXT | STORED | JSON metadata |\n\n**Minimal schema**: 6 fields vs CASS's 14 fields\n\n#### Tokenizer Chain\n```rust\n(Default tokenizer from Tantivy) — no explicit configuration\n```\n- **Name**: `default`\n- **Token length limit**: Tantivy default (no RemoveLongFilter)\n- **Index record option**: `WithFreqsAndPositions` for text, `Basic` for prefix\n\n#### Edge N-gram Implementation\n**Function**: `generate_prefixes(text: &str) → String`\n- Processes up to 100 words (early termination)\n- Allocates `Vec<char>` per word and reuses buffer\n- **Min word length**: 2 characters\n- **Max prefix length**: 15 characters (smaller than CASS's 20)\n- **Output format**: Space-separated (e.g., \"he hel hell ... hello\")\n\n**Key optimization**: Character-based (not byte-based), reuses char buffer per word\n\n#### Query Parsing\n- **Phrase detection**: Explicit check `if has_phrase { trimmed.contains('\"') }`\n- **Conditional field selection**: \n  - If phrase: use only `text_field` (phrase queries fail on Basic-indexed prefix_field)\n  - If no phrase: use `text_field` + `prefix_field`\n- Standard `QueryParser::for_index()` with conditional fields\n\n#### Subject vs Body Boost\n- **Not implemented**; single `text` field only\n\n#### Type/Doc Filtering\n- `type` field (STRING/exact match)\n- `DocType` enum with `as_str()` conversion\n- Can filter via BooleanQuery on type\n\n#### Date/Timestamp Range Queries\n- `created_at` is I64 + INDEXED + STORED + FAST\n- Stored as Unix timestamp (seconds)\n\n#### Snippet Generation\n**Via Tantivy's SnippetGenerator**:\n```rust\nSnippetGenerator::create(&searcher, &query, text_field)\n  → generator.snippet_from_doc(&doc)\n  → snippet.to_html() // HTML with <em> tags\n```\n- Automatic context extraction\n- HTML highlighting built-in\n\n#### Index Creation/Opening\n- **Path-based**: `{index_path}/` with auto-create\n- **Meta.json check**: Exists → `Index::open_in_dir()`, else `Index::create_in_dir()`\n- **Reader reload policy**: `OnCommitWithDelay` for on-disk, `Manual` for in-memory\n- **In-memory support**: `Index::create_in_ram()` for testing\n\n#### Document Addition (Batch)\n- One writer per document type (`index_tweets()`, `index_likes()`, etc.)\n- Batch operations: loop over slice and `add_document()` for each\n- Single `writer.commit()` after batch\n\n#### Thread Safety\n- Single `IndexWriter` per instance\n- `SearchEngine` has `reader: IndexReader` (thread-safe for read)\n- No explicit concurrency wrapper shown\n\n---\n\n### PROJECT 3: MCP_AGENT_MAIL (mcp-agent-mail-search-core)\n\n#### Schema Definition\n| Field | Type | Options | Purpose |\n|-------|------|---------|---------|\n| `id` | U64 | INDEXED + STORED + FAST | Document ID (u64) |\n| `doc_kind` | TEXT | STRING + STORED + FAST | Document type (message/agent/project) |\n| `subject` | TEXT | TEXT + STORED | Subject/title |\n| `body` | TEXT | TEXT + STORED | Message body |\n| `sender` | TEXT | STRING + STORED + FAST | Sender agent name |\n| `project_slug` | TEXT | STRING + STORED + FAST | Project identifier |\n| `project_id` | U64 | INDEXED + STORED + FAST | Project ID |\n| `thread_id` | TEXT | STRING + STORED + FAST | Thread identifier |\n| `importance` | TEXT | STRING + STORED + FAST | Priority level (low/normal/high/urgent) |\n| `created_ts` | I64 | INDEXED + STORED + FAST | Timestamp (microseconds since epoch) |\n| `program` | TEXT | STORED | Program name (agents only) |\n| `model` | TEXT | STORED | Model name (agents only) |\n\n**Total**: 12 fields (middle ground between CASS and XF)\n\n#### Tokenizer Chain\n```rust\nSimpleTokenizer → LowerCaser → RemoveLongFilter::limit(256)\n```\n- **Name**: `am_default`\n- **Token length limit**: 256 bytes (same as CASS)\n- **Index record option**: `WithFreqsAndPositions` for subject/body\n\n#### Edge N-gram Implementation\n- **NOT IMPLEMENTED** in tantivy_schema.rs; no prefix fields defined\n- Prefix matching must be done at query time via regex or wildcard\n\n#### Query Parsing (lexical_parser.rs)\n**Sophisticated sanitization + dual-stage parsing**:\n\n1. **Sanitization** (`sanitize_query()`):\n   - Escape special Tantivy chars: `[ ] { } ^ ~ \\`\n   - Replace parentheses with spaces\n   - Collapse multiple spaces\n   - Strip leading wildcards\n   - Strip trailing lone wildcard\n   - Quote hyphenated tokens: `POL-358` → `\"POL-358\"`\n\n2. **Structured hints** (`parse_query_assistance()`):\n   - **Canonical fields**: `from`, `thread`, `project`, `before`, `after`, `importance`\n   - **Aliases**: `sender→from`, `thread_id→thread`, `proj→project`, `since→after`, `until→before`, etc.\n   - **Typo-tolerant**: Levenshtein distance ≤2 for \"did you mean\" suggestions\n\n3. **Fallback strategy** (on primary parse failure):\n   - Extract plain terms via `extract_terms()`\n   - Build OR-joined BooleanQuery\n\n#### Subject vs Body Boost\n- **SUBJECT_BOOST**: `2.0` (constant)\n- **BODY_BOOST**: `1.0` (baseline)\n- Applied at query time via `BoostQuery` wrapper (not at index time)\n\n#### Type/Doc Kind Filtering\n- `doc_kind` field (STRING/exact match)\n- Values: `\"message\"`, `\"agent\"`, `\"project\"`\n- Filtered at query time via TermQuery\n\n#### Date/Timestamp Range Queries\n- `created_ts` is I64 + INDEXED + FAST\n- **Microseconds** (vs CASS's milliseconds, XF's seconds)\n- Range queries via BooleanQuery with comparison predicates\n\n#### Snippet Generation\n**Via** `lexical_response.rs`:\n```rust\ngenerate_snippet(text: &str, query_terms: &[String]) → Option<String>\n```\n- Searches for first query term match (case-insensitive)\n- Extracts context window (40 chars before/after)\n- Snaps to word boundaries\n- **Max snippet length**: 200 characters\n- **Highlighting**: No explicit highlighting; snippet only\n\n**Highlight ranges**:\n```rust\nfind_highlights(text: &str, field_name: &str, query_terms: &[String]) → Vec<HighlightRange>\n```\n- Case-insensitive term search\n- Byte-offset ranges for all occurrences\n- Sorted by position\n\n#### Index Creation/Opening\n- **Path-based**: Uses versioned directory with schema hash\n- **Schema hash**: SHA256 of schema definition + tokenizer config + field names\n- **Hash check**: Triggers full reindex on mismatch\n- **Deterministic**: Field order sorting ensures consistent hash\n\n#### Document Addition (Batch)\n- Uses `writer.add_document(doc!(...))` macro\n- Batch loop for multiple documents\n- Explicit `commit()` required\n\n#### Thread Safety\n- Single `IndexWriter` instance per search session\n- No explicit concurrency mentioned; likely protected by caller\n\n---\n\n## COMPARISON TABLE: BEST PRACTICES\n\n| Aspect | CASS | XF | MCP_AGENT_MAIL | FRANKENSEARCH RECOMMENDATION |\n|--------|------|----|----|------|\n| **Tokenizer** | SimpleTokenizer + LowerCaser + RemoveLongFilter(256) | Default (implicit) | SimpleTokenizer + LowerCaser + RemoveLongFilter(256) | **Use CASS/MCP pattern**: explicit 256-byte limit |\n| **Token Length Limit** | 256 bytes | Implicit default | 256 bytes | **256 bytes** (protects against pathological input) |\n| **N-gram Min Length** | 2 chars | 2 chars | None (no prefixes) | **2 chars** (skip single-char words) |\n| **N-gram Max Length** | 20 chars | 15 chars | N/A | **15-20 chars**: XF's 15 is safer for memory; CASS's 20 for better recall. **Adopt 16 (compromise)** |\n| **N-gram Data Structure** | ArrayVec (stack, 21 slots) | Vec<char> buffer reuse | N/A | **ArrayVec**: Zero heap allocation for millions of terms |\n| **N-gram Splitting** | Non-alphanumeric chars | Non-alphanumeric chars | N/A | **Non-alphanumeric splitting** (standard) |\n| **Index Record Option** | WithFreqsAndPositions | WithFreqsAndPositions (text), Basic (prefix) | WithFreqsAndPositions | **WithFreqsAndPositions**: Enables phrase queries |\n| **Phrase Query Handling** | Implicit (default Tantivy) | **Explicit**: detect `\"` and exclude prefix_field | Not shown | **Explicit phrase detection** (XF pattern): avoid phrase queries on Basic-indexed prefix field |\n| **Prefix Field Indexing** | Basic (implicit via generate_edge_ngrams) | Basic (explicit IndexRecordOption::Basic) | N/A | **Basic for prefix fields** (no position data needed) |\n| **Subject/Body Boost** | Not implemented | Not implemented | **2.0 / 1.0** (query-time BoostQuery) | **Adopt MCP pattern**: 2.0 for subject, 1.0 for body, applied at query time |\n| **Schema Versioning** | String constant comparison | Meta.json existence check | SHA256 hash (deterministic) | **Use MCP pattern**: SHA256 of schema + tokenizer = deterministic rebuild trigger |\n| **Schema Fields** | 14 (comprehensive) | 6 (minimal) | 12 (balanced) | **Adopt MCP (12-14 fields)**: Include provenance/metadata but not excessive |\n| **Field Types** | Mixed (TEXT, STRING, U64, I64) | Mixed | Mixed | **Use STRING for exact match** (agent, doc_kind, type), **TEXT for full-text** |\n| **FAST Fields** | created_at only | created_at only | created_at, doc_kind, sender, project_id, project_slug, thread_id, importance | **Mark all sort/range fields as FAST**: For efficient sorting and range queries |\n| **Type Filtering** | Via agent/origin_kind (STRING) | Via type (STRING) | Via doc_kind (STRING) | **Use STRING + FAST for doc type** filtering |\n| **Timestamp Precision** | Milliseconds | Seconds | **Microseconds** | **Use microseconds** (MCP): Finer granularity, future-proof |\n| **Query Parsing** | Standard QueryParser | Standard + explicit phrase check | Sanitization + structured hints + fallback | **Adopt MCP pattern**: Sanitize first, then parse, with fallback to OR-joined terms |\n| **Query Sanitization** | None shown | None shown | **Yes** (escape special chars, collapse spaces, quote hyphens) | **Always sanitize** before parsing |\n| **Structured Hints** | None | None | **Yes** (from:, thread:, project:, etc. with aliases) | **Optional for app**, but valuable for power users |\n| **Snippet Generation** | Plain text truncation (400 chars) | Tantivy's SnippetGenerator (HTML with em tags) | Custom (200 chars, word-boundary snapping, term highlighting) | **Adopt MCP/XF hybrid**: Tantivy SnippetGenerator for HTML, with custom context snapping |\n| **Highlighting** | Via preview field only | Via Tantivy SnippetGenerator | Via find_highlights() byte offsets | **Use find_highlights()**: Byte-offset ranges, deterministic |\n| **Index Rebuild Trigger** | Schema hash mismatch | Meta.json absence | Schema hash SHA256 mismatch | **Schema hash comparison** (MCP pattern) |\n| **Index Locking** | Tantivy default (auto) | Tantivy default (auto) | Tantivy default (auto) | **Rely on Tantivy's locking** |\n| **Segment Merge** | Async with cooldown (5 min, 4 seg threshold) | Not shown | Not shown | **Async merge with cooldown** (CASS pattern): Reduces read latency spikes |\n| **Merge Cooldown** | 300 seconds (5 min) | N/A | N/A | **300 seconds** (CASS pattern) |\n| **Merge Threshold** | 4 segments | N/A | N/A | **4 segments** (CASS pattern) |\n| **Writer Heap Size** | 50MB | Not specified | 15MB (in tests) | **Start with 15-50MB**, tune based on document size |\n| **Document Batch Size** | Per conversation (variable) | Per type (variable) | Implicit (per document) | **Batch by logical unit** (conversation/thread), explicit commit after batch |\n| **Error Handling** | Rebuild on corruption (remove + recreate) | Schema mismatch via open fail | Hash-based rebuild | **Adopt CASS pattern**: Remove corrupt directory, full rebuild as last resort |\n\n---\n\n## FRANKENSEARCH ADOPTION BLUEPRINT\n\nBased on best-of-breed patterns:\n\n### 1. Schema Structure\n```rust\n#[derive(Clone, Copy)]\npub struct Fields {\n    pub id: Field,                      // U64 INDEXED STORED FAST\n    pub doc_kind: Field,                // STRING STORED FAST (message/chunk/doc)\n    pub subject: Field,                 // TEXT STORED (title/heading)\n    pub body: Field,                    // TEXT STORED (main content)\n    pub created_ts: Field,              // I64 INDEXED STORED FAST (microseconds)\n    pub subject_prefix: Field,          // TEXT (no STORED, BasicIndex)\n    pub body_prefix: Field,             // TEXT (no STORED, BasicIndex)\n    pub document_id: Field,             // STRING STORED FAST (external ID)\n    pub source: Field,                  // STRING STORED FAST (source type)\n    pub metadata: Field,                // TEXT STORED (JSON)\n}\n```\n\n### 2. Tokenizer Registration\n```rust\npub fn register_tokenizer(index: &Index) {\n    let analyzer = TextAnalyzer::builder(SimpleTokenizer::default())\n        .filter(LowerCaser)\n        .filter(RemoveLongFilter::limit(256))\n        .build();\n    index.tokenizers().register(\"fs_default\", analyzer);\n}\n```\n\n### 3. N-gram Generation (CASS pattern with XF optimizations)\n```rust\nfn generate_edge_ngrams(text: &str) -> String {\n    let mut ngrams = String::with_capacity(text.len() * 2);\n    for word in text.split(|c: char| !c.is_alphanumeric()) {\n        let indices: ArrayVec<usize, 17> = word\n            .char_indices()\n            .map(|(i, _)| i)\n            .chain(std::iter::once(word.len()))\n            .take(17)\n            .collect();\n        \n        if indices.len() < 3 { continue; }\n        for &end_idx in &indices[2..] {\n            if !ngrams.is_empty() { ngrams.push(' '); }\n            ngrams.push_str(&word[..end_idx]);\n        }\n    }\n    ngrams\n}\n```\n- Min: 2 chars, Max: 16 chars (compromise of 15 and 20)\n- ArrayVec: No heap allocation\n- Max 16 indices (2-char + padding)\n\n### 4. Query Parsing (MCP pattern)\n```rust\npub fn parse_lexical_query(raw: &str, index: &Index) -> Result<Box<dyn Query>> {\n    // Step 1: Sanitize\n    let sanitized = sanitize_query(raw)?;\n    let query_text = sanitized.as_str().ok_or(anyhow!(\"empty query\"))?;\n    \n    // Step 2: Try primary parse\n    let parser = QueryParser::for_index(\n        index,\n        vec![subject_field, body_field], // Only full-text fields\n    );\n    \n    match parser.parse_query(query_text) {\n        Ok(q) => Ok(Box::new(apply_field_boost(q, subject_boost: 2.0, body_boost: 1.0))),\n        Err(e) => {\n            // Step 3: Fallback\n            let terms = extract_terms(query_text);\n            build_or_query(terms)\n        }\n    }\n}\n```\n\n### 5. Snippet Generation (MCP + Tantivy hybrid)\n```rust\npub fn generate_hit_snippet(doc: &TantivyDocument, query_terms: &[String]) -> Option<String> {\n    let text = doc.get_first(body_field)?.as_str()?;\n    generate_snippet(text, query_terms, 200, 40) // length, context\n}\n```\n- 200 char max\n- 40 char context before/after\n- Word-boundary snapping\n- Separate `find_highlights()` for byte-offset ranges\n\n### 6. Schema Versioning (MCP pattern)\n```rust\npub fn schema_hash() -> String {\n    let (schema, _) = build_schema();\n    let mut entries: Vec<String> = schema\n        .fields()\n        .map(|(f, e)| format!(\"{}:{:?}:{}\", e.name(), e.field_type(), f.field_id()))\n        .collect();\n    entries.sort();\n    \n    let mut hasher = Sha256::new();\n    hasher.update(SCHEMA_VERSION.as_bytes());\n    hasher.update(b\"\\n\");\n    for entry in &entries {\n        hasher.update(entry.as_bytes());\n        hasher.update(b\"\\n\");\n    }\n    hex::encode(hasher.finalize())\n}\n```\n\n### 7. Index Open/Create\n```rust\npub fn open_or_create(path: &Path) -> Result<Self> {\n    let schema = build_schema();\n    let hash_path = path.join(\"schema_hash.json\");\n    let mut needs_rebuild = true;\n    \n    if hash_path.exists() {\n        if let Ok(content) = std::fs::read_to_string(&hash_path) {\n            if let Ok(json) = serde_json::from_str::<serde_json::Value>(&content) {\n                if json.get(\"schema_hash\").and_then(|v| v.as_str()) == Some(&schema_hash()) {\n                    needs_rebuild = false;\n                }\n            }\n        }\n    }\n    \n    if needs_rebuild {\n        let _ = std::fs::remove_dir_all(path);\n        std::fs::create_dir_all(path)?;\n    }\n    \n    let index = if path.join(\"meta.json\").exists() && !needs_rebuild {\n        Index::open_in_dir(path)?\n    } else {\n        Index::create_in_dir(path, schema)?\n    };\n    \n    register_tokenizer(&index);\n    std::fs::write(&hash_path, format!(r#\"{{\"schema_hash\":\"{}\"}}\"#, schema_hash()))?;\n    \n    let writer = index.writer(50_000_000)?;\n    Ok(TantivyIndex { index, writer })\n}\n```\n\n### 8. Segment Management (CASS pattern)\n```rust\npub fn optimize_if_idle(&mut self) -> Result<bool> {\n    let segment_count = self.index.searchable_segment_ids()?.len();\n    \n    // Check threshold\n    if segment_count < 4 { return Ok(false); }\n    \n    // Check cooldown\n    let now_ms = SystemTime::now().duration_since(UNIX_EPOCH)?.as_millis() as i64;\n    let last_merge = LAST_MERGE_TS.load(Ordering::Relaxed);\n    if last_merge > 0 && (now_ms - last_merge) < 300_000 {\n        return Ok(false);\n    }\n    \n    let segment_ids = self.index.searchable_segment_ids()?;\n    let _ = self.writer.merge(&segment_ids); // Async\n    LAST_MERGE_TS.store(now_ms, Ordering::Relaxed);\n    Ok(true)\n}\n```\n\n---\n\n## SUMMARY: KEY DECISIONS FOR FRANKENSEARCH\n\n| Decision | Rationale |\n|----------|-----------|\n| **Use explicit tokenizer registration** | Ensures 256-byte limit + consistent lowercasing across all operations |\n| **ArrayVec for n-gram indices** | Zero allocation overhead; critical for millions of indexed documents |\n| **16-char max n-gram length** | Balances recall (longer prefixes) vs. memory (CASS's 20 is excessive) |\n| **Query-time field boost (2.0x subject)** | MCP pattern avoids index-time scoring; runtime flexibility |\n| **SHA256 schema hash** | Deterministic rebuild trigger; survives binary updates |\n| **Phrase detection + dual-field parsing** | XF pattern: exclude Basic-indexed prefix field from phrase queries |\n| **Sanitization before parsing** | MCP pattern: consistent fallback behavior |\n| **Async merge with 5-min cooldown** | CASS pattern: prevents latency spikes from segment merges |\n| **Microsecond timestamps** | MCP pattern: future-proof, matches RRF fusion granularity |\n| **Snippet + separate highlight ranges** | Hybrid: Tantivy for snippet context, custom byte offsets for rich highlighting |\n| **String type for exact-match filters** | Prevents tokenization of IDs/slugs (e.g., agent names, document kinds) |\n\n---\n\n## ADOPTION CHECKLIST FOR BD-3UN.2+\n\n- [ ] **bd-3un.2** (error types): Define `TantivyError` with schema hash, tokenizer, n-gram variants\n- [ ] **bd-3un.3-8** (document types): Use 12-field schema with `doc_kind` discrimination\n- [ ] **bd-3un.9-11** (indexing): Batch add with ArrayVec n-grams, explicit commit per batch\n- [ ] **bd-3un.12-17** (query): Sanitize + parse + boost + fallback pipeline\n- [ ] **bd-3un.18-20** (response): Snippet + highlight ranges, deterministic tie-breaking\n- [ ] **bd-3un.21-25** (schema): SHA256 versioning, tokenizer registration, FAST field marking\n- [ ] **bd-3un.31** (unit tests): All patterns from mcp_agent_mail tantivy_schema test suite\n- [ ] **bd-3un.32** (integration): Test phrase queries, prefix matching, field boosting, fallback\n- [ ] **bd-3un.39** (tracing): Log schema hash, segment merges, query parse mode (primary vs fallback)",
      "score": 36.203895568847656,
      "source_path": "/home/ubuntu/.claude/projects/-data-projects-frankensearch/90a3ac27-d05a-4a76-9e42-2c27aa78a955/subagents/agent-aec1842.jsonl",
      "agent": "claude_code",
      "workspace": "/data/projects/frankensearch",
      "created_at": 1771014566577,
      "line_number": 22,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    },
    {
      "title": "# AGENTS.md instructions for /data/projects/coding_agent_session_search",
      "snippet": "**Inspecting **Tantivy** **index** schema**\n\nI&#x27;m reviewing the top of src/search/**tantivy**.rs to identify the schema fields and understand the **index** structure",
      "content": "**Inspecting Tantivy index schema**\n\nI'm reviewing the top of src/search/tantivy.rs to identify the schema fields and understand the index structure.",
      "score": 34.791934967041016,
      "source_path": "/home/ubuntu/.codex/sessions/2026/01/25/rollout-2026-01-25T17-56-06-019bf75f-248c-7502-8cee-926d2ef33c66.jsonl",
      "agent": "codex",
      "workspace": "/data/projects/coding_agent_session_search",
      "created_at": 1769381891784,
      "line_number": 34,
      "match_type": "exact",
      "source_id": "local",
      "origin_kind": "local"
    }
  ],
  "max_tokens": null,
  "request_id": null,
  "cursor": null,
  "hits_clamped": false
}
